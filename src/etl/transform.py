"""MÃ³dulo de transformaciÃ³n de datos meteorolÃ³gicos (Transform)."""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional

from ..utils import Config, setup_logger
from .validators import DataValidator


config = Config()
logger = setup_logger(
    "etl.transform",
    log_file=config.reports_dir / "etl.log",
    level=config.log_level,
    format_type=config.log_format
)


class DataMerger:
    """Consolidador de mÃºltiples variables en un Ãºnico DataFrame."""
    
    @staticmethod
    def merge_variables(dfs: Dict[str, pd.DataFrame]) -> pd.DataFrame:
        """
        Merge de mÃºltiples DataFrames de variables por timestamp.
        
        Args:
            dfs: Diccionario {nombre_variable: DataFrame}
        
        Returns:
            DataFrame consolidado
        """
        logger.info(f"ğŸ”— Consolidando {len(dfs)} variables")
        
        if not dfs:
            raise ValueError("No hay DataFrames para consolidar")
        
        df_merged = None
        
        for var_name, df in dfs.items():
            if df_merged is None:
                df_merged = df.copy()
                logger.debug(f"  Base: {var_name} ({len(df)} registros)")
            else:
                df_merged = pd.merge(
                    df_merged,
                    df,
                    on="timestamp",
                    how="outer"
                )
                logger.debug(f"  + {var_name} ({len(df)} registros)")
        
        df_merged = df_merged.sort_values("timestamp").reset_index(drop=True)
        
        logger.info(
            f"âœ… Consolidado: {len(df_merged)} registros, "
            f"{len(df_merged.columns) - 1} variables"
        )
        logger.info(
            f"ğŸ“… Rango: {df_merged['timestamp'].min()} â†’ {df_merged['timestamp'].max()}"
        )
        
        return df_merged


class DataCleaner:
    """Limpiador y validador de datos."""
    
    def __init__(self, validator: DataValidator = None):
        """
        Inicializa el limpiador.
        
        Args:
            validator: Validador personalizado (opcional)
        """
        self.validator = validator or DataValidator()
    
    def clean_and_validate(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Limpieza y validaciÃ³n completa de datos.
        
        Args:
            df: DataFrame a limpiar
        
        Returns:
            DataFrame limpio y validado
        """
        logger.info("ğŸ§¹ Iniciando limpieza y validaciÃ³n")
        
        initial_rows = len(df)
        
        # 1. Eliminar duplicados
        df = self._remove_duplicates(df)
        
        # 2. Validar rangos
        df = self._validate_ranges(df)
        
        # 3. Limpiar outliers
        df = self._clean_outliers(df)
        
        # 4. Validar consistencia temporal
        self._check_temporal_consistency(df)
        
        final_rows = len(df)
        logger.info(
            f"âœ… Limpieza completada: {initial_rows} â†’ {final_rows} registros "
            f"({initial_rows - final_rows} eliminados)"
        )
        
        return df
    
    def _remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        """Elimina timestamps duplicados."""
        duplicates = df["timestamp"].duplicated().sum()
        
        if duplicates > 0:
            logger.warning(f"âš ï¸ Eliminando {duplicates} timestamps duplicados")
            df = df.drop_duplicates(subset=["timestamp"], keep="first")
        
        return df
    
    def _validate_ranges(self, df: pd.DataFrame) -> pd.DataFrame:
        """Valida rangos de variables."""
        validation_result = self.validator.validate_dataframe(df)
        
        for error in validation_result.errors:
            logger.error(f"âŒ {error}")
        
        for warning in validation_result.warnings:
            logger.warning(f"âš ï¸ {warning}")
        
        if not validation_result.valid:
            raise ValueError("FallÃ³ la validaciÃ³n de datos")
        
        return df
    
    def _clean_outliers(self, df: pd.DataFrame, method: str = "clip") -> pd.DataFrame:
        """Limpia outliers extremos."""
        df_clean, modified = self.validator.clean_outliers(df, method=method)
        
        if modified > 0:
            logger.info(f"ğŸ”§ Corregidos {modified} valores fuera de rango")
        
        return df_clean
    
    def _check_temporal_consistency(self, df: pd.DataFrame) -> None:
        """Verifica consistencia temporal."""
        temporal_result = self.validator.check_temporal_consistency(df)
        
        for warning in temporal_result.warnings:
            logger.warning(f"â° {warning}")


class DataTransformer:
    """Transformador principal de datos."""
    
    def __init__(self):
        """Inicializa el transformador."""
        self.merger = DataMerger()
        self.cleaner = DataCleaner()
    
    def transform(
        self,
        extracted_data: Dict[str, pd.DataFrame],
        fill_gaps: bool = True,
        freq: str = "H"
    ) -> pd.DataFrame:
        """
        Pipeline completo de transformaciÃ³n.
        
        Args:
            extracted_data: Datos extraÃ­dos {variable: DataFrame}
            fill_gaps: Completar gaps temporales
            freq: Frecuencia temporal (default: 'H' para horaria)
        
        Returns:
            DataFrame transformado y limpio
        """
        logger.info("="*60)
        logger.info("TRANSFORMACIÃ“N DE DATOS (TRANSFORM)")
        logger.info("="*60)
        
        # 1. Merge de variables
        df = self.merger.merge_variables(extracted_data)
        
        # 2. Completar gaps temporales si es necesario
        if fill_gaps:
            df = self._fill_temporal_gaps(df, freq)
        
        # 3. Limpieza y validaciÃ³n
        df = self.cleaner.clean_and_validate(df)
        
        # 4. Reporte de calidad
        self._quality_report(df)
        
        logger.info("="*60)
        
        return df
    
    def _fill_temporal_gaps(self, df: pd.DataFrame, freq: str) -> pd.DataFrame:
        """
        Completa gaps temporales con frecuencia especificada.
        
        Args:
            df: DataFrame con timestamp
            freq: Frecuencia ('H', 'D', etc.)
        
        Returns:
            DataFrame con frecuencia completa
        """
        original_len = len(df)
        
        # Establecer timestamp como Ã­ndice
        df = df.set_index("timestamp").sort_index()
        
        # Completar frecuencia
        df = df.asfreq(freq)
        
        # Resetear Ã­ndice
        df = df.reset_index()
        
        if len(df) > original_len:
            added = len(df) - original_len
            logger.warning(
                f"âš ï¸ Completada frecuencia {freq}: {original_len} â†’ {len(df)} "
                f"({added} registros aÃ±adidos con NaN)"
            )
        
        return df
    
    def _quality_report(self, df: pd.DataFrame) -> None:
        """Genera reporte de calidad de datos."""
        logger.info("ğŸ“Š Reporte de calidad:")
        logger.info(f"  â€¢ Registros totales: {len(df)}")
        logger.info(f"  â€¢ Variables: {len(df.columns) - 1}")
        logger.info(f"  â€¢ Columnas: {list(df.columns)}")
        
        # Valores nulos
        null_counts = df.isnull().sum()
        if null_counts.sum() > 0:
            logger.warning("  â€¢ Valores nulos:")
            for col, count in null_counts[null_counts > 0].items():
                pct = (count / len(df)) * 100
                logger.warning(f"    - {col}: {count} ({pct:.2f}%)")
        else:
            logger.info("  â€¢ Sin valores nulos âœ…")
        
        # EstadÃ­sticas bÃ¡sicas
        logger.info("  â€¢ EstadÃ­sticas:")
        for col in df.columns:
            if col != "timestamp" and pd.api.types.is_numeric_dtype(df[col]):
                logger.info(
                    f"    - {col}: "
                    f"min={df[col].min():.2f}, "
                    f"max={df[col].max():.2f}, "
                    f"mean={df[col].mean():.2f}"
                )


class FeatureEngineer:
    """Ingeniero de caracterÃ­sticas (opcional en ETL)."""
    
    @staticmethod
    def add_basic_features(df: pd.DataFrame) -> pd.DataFrame:
        """
        AÃ±ade caracterÃ­sticas bÃ¡sicas temporales.
        
        Args:
            df: DataFrame con timestamp
        
        Returns:
            DataFrame con features bÃ¡sicos
        """
        df = df.copy()
        
        # Asegurar que timestamp es datetime
        if not pd.api.types.is_datetime64_any_dtype(df["timestamp"]):
            df["timestamp"] = pd.to_datetime(df["timestamp"])
        
        # Features temporales bÃ¡sicos
        df["hour"] = df["timestamp"].dt.hour
        df["day_of_week"] = df["timestamp"].dt.dayofweek
        df["month"] = df["timestamp"].dt.month
        df["is_weekend"] = df["day_of_week"].isin([5, 6]).astype(int)
        
        logger.info("âœ… Features temporales bÃ¡sicos aÃ±adidos")
        
        return df
    
    @staticmethod
    def filter_columns(
        df: pd.DataFrame,
        exclude_patterns: List[str] = None
    ) -> pd.DataFrame:
        """
        Filtra columnas segÃºn patrones.
        
        Args:
            df: DataFrame
            exclude_patterns: Patrones a excluir (ej: ['10m'])
        
        Returns:
            DataFrame filtrado
        """
        if exclude_patterns is None:
            return df
        
        cols_to_drop = []
        for pattern in exclude_patterns:
            cols_to_drop.extend([col for col in df.columns if pattern in col])
        
        if cols_to_drop:
            df = df.drop(columns=cols_to_drop)
            logger.info(f"ğŸ—‘ï¸ Eliminadas columnas: {cols_to_drop}")
        
        return df
